{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task - Implement Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 - Implement Logistic Regression Model\n",
    "import numpy as np\n",
    "class LogisticReg():\n",
    "    def __init__(self,\n",
    "                learning_rate=0.01,\n",
    "                threshold=0.5, \n",
    "                num_iterations=1000,\n",
    "                verbose = False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.threshold = threshold\n",
    "        self.num_iterations = num_iterations\n",
    "        self.__n_features = None\n",
    "        self.__N_samples = None\n",
    "        self._weights = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __initialize(self, X, with_bias=False):\n",
    "        if(with_bias):\n",
    "            return X\n",
    "        else:\n",
    "            return np.c_[X, np.ones(X.shape[0])]\n",
    "         \n",
    "    def fit(self, X, y, with_bias=False) -> None:\n",
    "        self.__N_samples, self.__n_features = X.shape\n",
    "        self._weights = np.zeros(self.__n_features+1)\n",
    "        X = self.__initialize(X, with_bias=with_bias) # add constant (bias) to X if needed\n",
    "        # gradient decent\n",
    "        for i in range(self.num_iterations):\n",
    "            y_prob = self.__sigmoid(np.dot(X, self._weights)) # predict the probabilities\n",
    "            gradient = (1/self.__N_samples) * np.dot(X.T, (y_prob - y))\n",
    "            self._weights = self._weights - self.learning_rate*gradient # learning progress\n",
    "            if self.verbose and (i % 25 == 0 or i == self.num_iterations-1): # print logloss if verbose is true\n",
    "                loss = self.log_loss(y_prob,y)\n",
    "                print(f\"Loss after iteration {i}: {loss}\")\n",
    "            \n",
    "    def predict_proba(self, X, with_bias=False):\n",
    "        if(with_bias == False):\n",
    "            X = self.__initialize(X, with_bias)  # add constance to X if needed\n",
    "        return self.__sigmoid(np.dot(X, self._weights))\n",
    "    \n",
    "    def predict(self, X, with_bias=False,threshold=None):\n",
    "        y_prob = self.predict_proba(X, with_bias) # predict probabilities\n",
    "        if threshold is None:\n",
    "            y_pred = np.where(y_prob < self.threshold, 0, 1) # return below 0.5 as 0 and upper as 1\n",
    "        else:\n",
    "            y_pred = np.where(y_prob < threshold, 0, 1)\n",
    "        return y_pred\n",
    "        \n",
    "    def score(self, X, y,with_bias=False,threshold=None) -> float: # check accuracy score\n",
    "        y_pred = self.predict(X,with_bias=with_bias,threshold=threshold) # predict labels\n",
    "        correct = np.sum(y == y_pred) # checking right predictions\n",
    "        return correct/len(y) # take the average of correct predictions\n",
    "    \n",
    "    def f1_score(self,y_true,y_pred):\n",
    "        # create confusion table (true positive, false negative and false positive)\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1)) # true positive\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1)) # false positive\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0)) # false negative\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return {'recall':recall,'precision':precision,'F1':f1}\n",
    "    \n",
    "    def __sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def log_loss(self, y_pred_prob, y):\n",
    "        epsilon = 1e-5  # to avoid from log(zero) adding epsilon\n",
    "        return (-y * np.log(y_pred_prob + epsilon) - (1 - y) * np.log(1 - y_pred_prob + epsilon)).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 - Run Logistic Regression Model on Dataset Spam/Ham - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to run only once\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 - Logistic Regression Model on Dataset Spam/Ham - Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 - Run Logistic Regression Model on Dataset Spam/Ham - Loading Data\n",
    "df = pd.read_csv('spam_ham_dataset.csv')\n",
    "df.drop(['Unnamed: 0'],axis=1,inplace = True)\n",
    "df.rename(columns={'label_num':'target'},inplace=True)\n",
    "df['text'] = df['text'].replace(r'\\n', ' ', regex=True) # remove \\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3 - Logistic Regression Model on Dataset Spam/Ham - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lens of text and show the information\n",
    "# working on duplicate because after looking on this details it isn't looking helpful for prediction\n",
    "df_dup = df.copy()\n",
    "# this part take few seconds of run time\n",
    "df_dup['num_characters'] = df_dup['text'].apply(len) # add more information about len of text\n",
    "df_dup['num_words'] = df_dup['text'].apply(lambda text:len(nltk.word_tokenize(text)))\n",
    "df_dup['num_sentences'] = df_dup['text'].apply(lambda text:len(nltk.sent_tokenize(text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot some data \n",
    "- this is not required but it is looking nice and help to understand a little more about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data informations\n",
    "print(\"Ham:\")\n",
    "print(df_dup[df_dup['target'] == 0][['num_characters','num_words','num_sentences']].describe())\n",
    "print(\"\\nSpam:\")\n",
    "print(df_dup[df_dup['target'] == 1][['num_characters','num_words','num_sentences']].describe())\n",
    "print(\"\\nBoth:\")\n",
    "print(df_dup[['num_characters','num_words','num_sentences']].describe())\n",
    "\n",
    "# plot the histogram of lens\n",
    "df_dup.hist(column='num_characters',bins=200,figsize=(10,4),)\n",
    "df_dup.hist(column='num_characters',by='label',bins=100,figsize=(10,4))\n",
    "df_dup.hist(column='num_words',bins=200,figsize=(10,4))\n",
    "df_dup.hist(column='num_words',by='label',bins=100,figsize=(10,4))\n",
    "df_dup.hist(column='num_sentences',bins=200,figsize=(10,4))\n",
    "df_dup.hist(column='num_sentences',by='label',bins=100,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(df_dup[df_dup['target'] == 0]['num_characters'])\n",
    "sns.histplot(df_dup[df_dup['target'] == 1]['num_characters'],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(df_dup[df_dup['target'] == 0]['num_words'])\n",
    "sns.histplot(df_dup[df_dup['target'] == 1]['num_words'],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(df_dup[df_dup['target'] == 0]['num_sentences'])\n",
    "sns.histplot(df_dup[df_dup['target'] == 1]['num_sentences'],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_dup,hue='target')\n",
    "df_withoutStrings = df_dup.drop(columns=['label','text'],inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show correlation between lens and label\n",
    "sns.heatmap(df_withoutStrings.corr(),annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating bag of words from dataset text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize) # for print the weight vector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(text):\n",
    "    # Split text by whitespace and remove punctuation with regex\n",
    "    tokens = re.findall(r'(?u)\\b\\w\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "# Vocabulary creation\n",
    "def create_vocab(corpus):\n",
    "    # Count occurrence of each word in the entire corpus\n",
    "    vocab_counts = Counter(word for text in corpus for word in text)\n",
    "    # Create vocabulary from the words\n",
    "    vocab = {word: i for i, (word, count) in enumerate(vocab_counts.most_common())}\n",
    "    return vocab\n",
    "\n",
    "# Counting word occurrences\n",
    "def count_words(text, vocab):\n",
    "    # Count occurrences of words in the text using a vocab\n",
    "    word_counts = np.zeros(len(vocab), dtype=np.int32)\n",
    "    for word in text:\n",
    "        if word in vocab:\n",
    "            word_counts[vocab[word]] += 1\n",
    "    return word_counts\n",
    "\n",
    "# Vectorization\n",
    "def vectorize(corpus, vocab):\n",
    "    N, n = len(corpus), len(vocab)\n",
    "    vectors = np.zeros(shape=(N,n), dtype=np.int32)\n",
    "    for i, text in enumerate(corpus):\n",
    "        # Count word occurrences in document using a numpy array\n",
    "        vector = count_words(text, vocab)\n",
    "        vectors[i] = vector\n",
    "    return vectors\n",
    "\n",
    "# Split the data for train and test\n",
    "X_train, X_test, y_trn, y_tst = train_test_split(df['text'], df['target'], train_size=0.8)\n",
    "\n",
    "# Create corpus to train and test\n",
    "corpus_X_train = [tokenize(text.lower()) for text in X_train] # Tokenize corpus from the text in X_train\n",
    "corpus_X_test = [tokenize(text.lower()) for text in X_test] # Tokenize corpus from the text in X_test\n",
    "\n",
    "# Create vocabulary from corpus of only from X_train\n",
    "vocab = create_vocab(corpus_X_train)\n",
    "\n",
    "# Vectorize X_train and X_test using vocabulary of X_train\n",
    "X_trn = vectorize(corpus_X_train, vocab)\n",
    "X_tst = vectorize(corpus_X_test, vocab)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the logistic regression model on the dataset and show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticReg(learning_rate=0.25,num_iterations=300,verbose=True) # explain below\n",
    "clf.fit(X_trn,y_trn)\n",
    "y_tst_pred = clf.predict(X_tst)\n",
    "# printing Scores for see the results of the model\n",
    "print('-------------- Scores --------------')\n",
    "print(f'Accuracy Score: {clf.score(X_tst,y_tst)}')\n",
    "print(f'F1 Score: {clf.f1_score(y_tst_pred,y_tst)}')\n",
    "print(f'Log Loss: {clf.log_loss(clf.predict_proba(X_tst),y_tst)}')\n",
    "print('------------------------------------')\n",
    "# plot confusion matrix\n",
    "cm = confusion_matrix(y_tst,y_tst_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['ham','spam'])\n",
    "disp.plot()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The weights vector:\\n{clf._weights}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation to choose parameters**\n",
    "- I shuffled (re-split and run) few times to see consistency of scores.\n",
    "-  For decide which predictions are good, i used accuracy score and F1 score with confustion table.\n",
    "    - I added verbose flag to the model for printing the log loss function of predictions to see the progress of learning.\n",
    "    - I choosed parameters when i got F1 Score (around 95%+) and that i saw that when i changed paramaters one of scores go up but the second go down, scores didn't rise significantly.\n",
    "- The learning rate is 0.25 after trying different values and see that lead to good results in most of tests.\n",
    "-  The num_iterations is 300 because less that 300 usually not enough for reach a good results and more is not increase the score in significant value and some times lead to unbalance score between recall and precision.\n",
    "- We will prefer maybe the less false positive predictions but it i didn't find one and yet the false positive predictions are relatively low.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4 - ROC Cruve on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_prob):\n",
    "    # Compute true positive rate (TPR) and false positive rate (FPR) for different thresholds\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    tpr_values = []\n",
    "    fpr_values = []\n",
    "    num_positive_cases = sum(y_true)\n",
    "    num_negative_cases = len(y_true) - num_positive_cases\n",
    "    for threshold in thresholds:\n",
    "        y_pred = np.where(y_prob < threshold, 0, 1)\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1)) # true positive\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1)) # false positive\n",
    "        tpr = tp / num_positive_cases # create tpr score (recall)\n",
    "        fpr = fp / num_negative_cases # create fpr score (missing rate)\n",
    "        # record the scores\n",
    "        tpr_values.append(tpr)\n",
    "        fpr_values.append(fpr)\n",
    "    # change the list to np array for aritmetic between arrays\n",
    "    tpr_values = np.array(tpr_values)\n",
    "    fpr_values = np.array(fpr_values)\n",
    "    # pickup the most the most successful theshold\n",
    "    good_index = np.argmax(tpr_values-fpr_values) # find the most successful theshold index\n",
    "    good_threshold = thresholds[good_index] # keep the value\n",
    "    # Plot the ROC curve\n",
    "    plt.plot(fpr_values, tpr_values, label=f'ROC Curve')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guessing')\n",
    "    plt.scatter(fpr_values, tpr_values,s=20, color='black')\n",
    "    plt.scatter(fpr_values[good_index], tpr_values[good_index], marker='o', facecolors='red', edgecolors='red', s=40)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    return good_threshold\n",
    "\n",
    "good_threshold= plot_roc_curve(y_tst,clf.predict_proba(X_tst))\n",
    "print(f'The good threshold: {good_threshold:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.46\n",
    "y_tst_pred = clf.predict(X_tst,threshold=threshold)\n",
    "# printing Scores for see the results of the model\n",
    "print('-------------- Scores --------------')\n",
    "print(f'Accuracy Score: {clf.score(X_tst,y_tst,threshold=threshold)}')\n",
    "print(f'F1 Score: {clf.f1_score(y_tst_pred,y_tst)}')\n",
    "print(f'Log Loss: {clf.log_loss(clf.predict_proba(X_tst),y_tst)}')\n",
    "print('------------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC Curve Conclusions**\n",
    "- In looking on the ROC Curve we can see the the most successful theshold in the interval [0,1] with 101 numbers is usualy around 0.45-0.5\n",
    "- The red mark show the place of the threshold on the plot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5 - Linear Programing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5 - Section 2 - Imports\n",
    "from scipy.optimize import linprog\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 - Linear Programing Data Generator**\n",
    "###### I left the datasets i created for this task to check my code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset1 - Linear line with noise\n",
    "\n",
    "# Define a linear line\n",
    "a, b = (2, 0)\n",
    "line = lambda x: a * x + b\n",
    "# Generate random points close to the line\n",
    "n_samples, noise_scale = 1000, 10.0\n",
    "x_1 = np.linspace(-10, 10, n_samples)\n",
    "x_2 = line(x_1) + np.random.normal(loc=0, scale=noise_scale, size=n_samples)\n",
    "# Create input features X and labels y\n",
    "X = np.column_stack((x_1, x_2))  # make X matrix of points\n",
    "y = np.where(x_2 >= line(x_1), 1, -1)  # change y represent labels\n",
    "\n",
    "# Flip a random subset of labels to be wrong\n",
    "num_wrong_labels = y[y==1].shape[0] - 50\n",
    "y_pos = y == 1\n",
    "idx = np.random.choice(np.where(y_pos)[0], num_wrong_labels, replace=False)\n",
    "y[idx] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset2 Simple Dataset\n",
    "X = np.array([[1,0],[2,1],[3,-1],[2,4],[1,3],[3,2],[-2,4]])\n",
    "y = np.array([-1,-1,-1,-1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset3 - logic gate dataset\n",
    "\n",
    "# Define the parameters of the dataset\n",
    "num_samples, noise_scale = 1000, 10.0\n",
    "low, high = -10.0, 10.0  # Range of points\n",
    "# Generate the data\n",
    "X = np.random.uniform(low=low, high=high, size=(num_samples, 2))\n",
    "# Add some noise to the labels\n",
    "y = np.logical_or(X[:, 0] > 0, X[:, 1] > 0).astype(int)\n",
    "y = np.where(y == 0, -1, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 - Linear Programing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, n= X.shape\n",
    "# Create A_ub, b_ub, c for LP\n",
    "obj = np.concatenate([np.zeros(n),np.ones(N)])  # vector c\n",
    "lhs_ineq = np.concatenate([-y*np.eye(N)@X,-np.eye(N)],axis=1) # matrix A\n",
    "rhs_ineq = np.array([-1] *N) # vector B\n",
    "#            bounds for points             bound for support vector\n",
    "bnd = [(-float(\"inf\"), float(\"inf\"))] *n + [(0, float(\"inf\"))] * N\n",
    "\n",
    "opt = linprog(c=obj,\n",
    "            A_ub=lhs_ineq,\n",
    "            b_ub=rhs_ineq, \n",
    "            bounds=bnd,\n",
    "            method=\"highs\")\n",
    "print(opt)  # print result of program"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 - Linear Programing - plot the results** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=matplotlib.colors.ListedColormap(['blue','red']))\n",
    "x_min, x_max, y_min, y_max = np.amin(X[:, 0]), np.amax(X[:, 0]), np.amin(X[:, 1]), np.amax(X[:, 1])\n",
    "\n",
    "lx = np.linspace(x_min-3, x_max+3, 100)\n",
    "w = opt.x\n",
    "ly = [ -w[0] * p / w[1] for p in lx]\n",
    "\n",
    "plt.plot(lx, ly,'k--')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
